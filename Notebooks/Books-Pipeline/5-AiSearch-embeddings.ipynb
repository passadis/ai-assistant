{"cells":[{"cell_type":"code","execution_count":null,"id":"952b0652-488d-4784-8137-f2259f7f0213","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","# %pip install azure-search-documents azure-search azure-core azure-search-documents==11.6.0b3\n","\n"]},{"cell_type":"markdown","id":"adc24797-f859-4cba-81d6-c33c78a572b9","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["**NEW SERIES**\n"]},{"cell_type":"markdown","id":"ed133835-ce51-4dc0-8e2c-0a1369ff041d","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["- **Connect to Azure Ai Search Index**\n","- **Configure Vector Search & Update**"]},{"cell_type":"code","execution_count":null,"id":"1d3cea20-d017-4b20-ab28-4cc87a5d08a6","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# %pip install azure-search-documents azure-search azure-core openai==0.28"]},{"cell_type":"code","execution_count":null,"id":"adefbefa-df7c-440f-a24d-5223468c3da8","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import openai\n","from azure.search.documents.indexes import SearchIndexClient\n","from azure.search.documents.indexes.models import (\n","    SimpleField, SearchFieldDataType, SearchField,\n","    VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile,\n","    SemanticConfiguration, SemanticPrioritizedFields, SemanticField, SemanticSearch\n",")\n","from azure.core.credentials import AzureKeyCredential\n","\n","# Configuration\n","openai.api_type = \"azure\"\n","openai.api_base = \"https://xxxxxxxxxxxxxx.openai.azure.com/\"\n","openai.api_version = \"2023-11-01\"\n","openai.api_key = \"xxxxxxxxxxxxxxxxxxxx\"\n","\n","search_service_name = \"xxxxxxxxxxxxxx\"\n","search_index_name = \"xxxxxxxxxxx-index\"\n","admin_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n","endpoint = f\"https://{search_service_name}.search.windows.net\"\n","\n","# Create a search index client\n","index_client = SearchIndexClient(endpoint=endpoint, credential=AzureKeyCredential(admin_key))\n","\n","# Retrieve the existing index\n","existing_index = index_client.get_index(search_index_name)\n","\n","# Define new fields if necessary (e.g., contentVector, searchContent)\n","new_fields = [\n","    SearchField(name=\"Embedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), \n","                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n","    SearchField(name=\"searchContent\", type=SearchFieldDataType.String, searchable=True)\n","]\n","\n","# Add new fields to the existing fields\n","fields = existing_index.fields\n","for new_field in new_fields:\n","    if new_field.name not in [field.name for field in fields]:\n","        fields.append(new_field)\n","\n","# Configure the vector search\n","vector_search = VectorSearch(\n","    algorithms=[\n","        HnswAlgorithmConfiguration(\n","            name=\"myHnsw\",\n","            parameters={\n","                \"m\": 8,\n","                \"efConstruction\": 800,\n","                \"efSearch\": 800,\n","                \"metric\": \"cosine\"\n","            }\n","        )\n","    ],\n","    profiles=[\n","        VectorSearchProfile(\n","            name=\"myHnswProfile\",\n","            algorithm_configuration_name=\"myHnsw\",\n","        )\n","    ]\n",")\n","\n","# Define semantic configuration\n","semantic_config = SemanticConfiguration(\n","    name=\"my-semantic-config\",\n","    prioritized_fields=SemanticPrioritizedFields(\n","        title_field=SemanticField(field_name=\"Title\"),\n","        keywords_fields=[SemanticField(field_name=\"Genres\")],\n","        content_fields=[SemanticField(field_name=\"searchContent\")]\n","    )\n",")\n","\n","# Create the semantic settings with the configuration\n","semantic_search = SemanticSearch(configurations=[semantic_config])\n","\n","# Update the search index with the new fields and configurations\n","existing_index.fields = fields\n","existing_index.vector_search = vector_search\n","existing_index.semantic_search = semantic_search\n","\n","result = index_client.create_or_update_index(existing_index)\n","print(f'Index {result.name} updated successfully')\n"]},{"cell_type":"markdown","id":"cdece52e-0c27-48c0-ba5e-e3374af8fbe3","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"source":["- **Create Embeddings (rate limit patience)**\n","- **Get Embeddings as JSON**\n","- **Upload to Azure AI Search**\n","- **Batch - Parallel**"]},{"cell_type":"code","execution_count":null,"id":"1ca5f8ae-17e2-42c9-aa45-074d12578395","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import openai\n","import json\n","import time\n","from azure.search.documents import SearchClient\n","from azure.core.credentials import AzureKeyCredential\n","import concurrent.futures\n","import os\n","\n","# Configuration\n","openai.api_type = \"azure\"\n","openai.api_base = \"https://xxxxxxxxxxxxxx.openai.azure.com/\"\n","openai.api_version = \"2024-05-01-preview\"\n","openai.api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n","deployment_id = \"text-embedding-ada-002\"\n","\n","search_service_name = \"xxxxxxxxxxxxxxxxxx\"\n","search_index_name = \"xxxxxxxxxxxx-index\"\n","admin_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n","endpoint = f\"https://{search_service_name}.search.windows.net\"\n","\n","# Initialize the search client\n","search_client = SearchClient(endpoint=endpoint, index_name=search_index_name, credential=AzureKeyCredential(admin_key))\n","\n","# Fetch all documents from the search index\n","results = search_client.search(search_text=\"*\", include_total_count=True)\n","documents = [doc for doc in results]\n","\n","# Function to generate embeddings for a batch of texts\n","def generate_embeddings_batch(texts, max_retries=7, backoff_factor=2):\n","    embeddings = []\n","    for text in texts:\n","        for attempt in range(max_retries):\n","            try:\n","                response = openai.Embedding.create(input=[text], deployment_id=deployment_id)\n","                embeddings.append(response['data'][0]['embedding'])\n","                break\n","            except openai.error.RateLimitError as e:\n","                if attempt < max_retries - 1:\n","                    wait_time = backoff_factor * (2 ** attempt)\n","                    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n","                    time.sleep(wait_time)\n","                else:\n","                    print(\"Max retries exceeded. Please try again later.\")\n","                    raise e\n","            except Exception as e:\n","                print(f\"Unexpected error: {e}\")\n","                raise e\n","        time.sleep(1)  # Add a delay between individual requests to reduce aggressiveness\n","    return embeddings\n","\n","def process_documents(documents, batch_size=5, max_workers=8):\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = {}\n","        for i in range(0, len(documents), batch_size):\n","            batch = documents[i:i + batch_size]\n","            texts = [f\"{doc['Title']} {doc['Author']} {doc['Genres']} Rating: {doc['Rating']}\" for doc in batch]\n","            future = executor.submit(generate_embeddings_batch, texts)\n","            futures[future] = (batch, texts, i)\n","\n","        for future in concurrent.futures.as_completed(futures):\n","            try:\n","                embeddings = future.result()\n","                batch, texts, start_index = futures[future]\n","                print(f\"Processing batch starting at index {start_index}\")\n","                for j, embedding in enumerate(embeddings):\n","                    documents[start_index + j]['Embedding'] = embedding\n","                    documents[start_index + j]['searchContent'] = texts[j]\n","                    \n","            except Exception as e:\n","                print(f\"Error processing batch: {e}\")    \n","\n","# Process documents to generate embeddings\n","process_documents(documents)\n","\n","# Ensure the output directory exists\n","output_dir = \"/lakehouse/default/Files/embeddings\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Save the documents with embeddings to a JSON file in the lakehouse\n","output_file = os.path.join(output_dir, \"bookVectors.json\")\n","with open(output_file, 'w') as file:\n","    json.dump(documents, file, indent=2)  # Adding indent parameter for pretty printing\n","\n","print(f\"Documents with embeddings saved to {output_file}\")\n"]},{"cell_type":"code","execution_count":null,"id":"31f5425f-8028-4180-b51f-85ba5f5555fb","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["from azure.search.documents import SearchClient\n","from azure.search.documents import SearchIndexingBufferedSender\n","\n","# Upload the documents with embeddings to the index\n","search_client = SearchClient(endpoint=endpoint, index_name=search_index_name, credential=AzureKeyCredential(admin_key))\n","\n","# Using SearchIndexingBufferedSender to upload the documents in batches optimized for indexing\n","with SearchIndexingBufferedSender(\n","    endpoint=endpoint,\n","    index_name=search_index_name,\n","    credential=AzureKeyCredential(admin_key),\n",") as batch_client:\n","    # Add upload actions for all documents\n","    with open(\"/lakehouse/default/Files/embeddings/bookVectors.json\", 'r') as file:\n","        documents = json.load(file)\n","        batch_client.upload_documents(documents=documents)\n","\n","print(f\"Uploaded {len(documents)} documents in total\")\n"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"83b65b13-7f82-4177-838c-f19a8134860b","default_lakehouse_name":"Datasets","default_lakehouse_workspace_id":"9750728a-936e-41b9-a6cd-1247d645f4c5","known_lakehouses":[{"id":"83b65b13-7f82-4177-838c-f19a8134860b"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
