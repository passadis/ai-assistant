{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed133835-ce51-4dc0-8e2c-0a1369ff041d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**The Notebook updates the Index**\n",
    "- **Connect to Azure Ai Search Index**\n",
    "- **Configure Vector Search & Update**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3cea20-d017-4b20-ab28-4cc87a5d08a6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# %pip install azure-search-documents azure-search azure-core openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefbefa-df7c-440f-a24d-5223468c3da8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField, SearchFieldDataType, SearchField,\n",
    "    VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile,\n",
    "    SemanticConfiguration, SemanticPrioritizedFields, SemanticField, SemanticSearch\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://xxxxx.openai.azure.com/\"\n",
    "openai.api_version = \"2023-11-01\"\n",
    "openai.api_key = \"xxxxxx\"\n",
    "\n",
    "search_service_name = \"xxxxxx\"\n",
    "search_index_name = \"books-index\"\n",
    "admin_key = \"xxxxxxx\"\n",
    "endpoint = f\"https://{search_service_name}.search.windows.net\"\n",
    "\n",
    "# Create a search index client\n",
    "index_client = SearchIndexClient(endpoint=endpoint, credential=AzureKeyCredential(admin_key))\n",
    "\n",
    "# Retrieve the existing index\n",
    "existing_index = index_client.get_index(search_index_name)\n",
    "\n",
    "# Define new fields if necessary (e.g., contentVector, searchContent)\n",
    "new_fields = [\n",
    "    SearchField(name=\"Embedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
    "    SearchField(name=\"searchContent\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchField(name=\"DescriptionEmbedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\")\n",
    "]\n",
    "\n",
    "# Add new fields to the existing fields\n",
    "fields = existing_index.fields\n",
    "for new_field in new_fields:\n",
    "    if new_field.name not in [field.name for field in fields]:\n",
    "        fields.append(new_field)\n",
    "\n",
    "# Configure the vector search\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\",\n",
    "            parameters={\n",
    "                \"m\": 8,\n",
    "                \"efConstruction\": 800,\n",
    "                \"efSearch\": 800,\n",
    "                \"metric\": \"cosine\"\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define semantic configuration\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"Title\"),\n",
    "        keywords_fields=[SemanticField(field_name=\"Genres\")],\n",
    "        content_fields=[SemanticField(field_name=\"searchContent\"), SemanticField(field_name=\"Description\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "# Update the search index with the new fields and configurations\n",
    "existing_index.fields = fields\n",
    "existing_index.vector_search = vector_search\n",
    "existing_index.semantic_search = semantic_search\n",
    "\n",
    "result = index_client.create_or_update_index(existing_index)\n",
    "print(f'Index {result.name} updated successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdece52e-0c27-48c0-ba5e-e3374af8fbe3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "- **Create Embeddings (rate limit patience)**\n",
    "- **Get Embeddings as JSON**\n",
    "- **Upload to Azure AI Search**\n",
    "- **Batch - Parallel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7cf705-308d-416d-acad-4d7d8972424c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import time\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://xxxxx.openai.azure.com/\"\n",
    "openai.api_version = \"2024-05-01-preview\"\n",
    "openai.api_key = \"xxxxxxx\"\n",
    "deployment_id = \"text-embedding-ada-002\"\n",
    "\n",
    "search_service_name = \"xxxxxx\"\n",
    "search_index_name = \"books-index\"\n",
    "admin_key = \"xxxxxxxx\"\n",
    "endpoint = f\"https://{search_service_name}.search.windows.net\"\n",
    "\n",
    "# Initialize the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=search_index_name, credential=AzureKeyCredential(admin_key))\n",
    "\n",
    "# Fetch all documents from the search index\n",
    "results = search_client.search(search_text=\"*\", include_total_count=True)\n",
    "documents = [doc for doc in results]\n",
    "\n",
    "# Function to generate embeddings for a batch of texts\n",
    "def generate_embeddings_batch(texts, max_retries=7, backoff_factor=2):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = openai.Embedding.create(input=[text], deployment_id=deployment_id)\n",
    "                embeddings.append(response['data'][0]['embedding'])\n",
    "                break\n",
    "            except openai.error.RateLimitError as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = backoff_factor * (2 ** attempt)\n",
    "                    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(\"Max retries exceeded. Please try again later.\")\n",
    "                    raise e\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "                raise e\n",
    "        time.sleep(1)  # Add a delay between individual requests to reduce aggressiveness\n",
    "    return embeddings\n",
    "\n",
    "def process_documents(documents, batch_size=5, max_workers=8):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {}\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            \n",
    "            # Create searchContent text\n",
    "            search_texts = [f\"{doc.get('Title', '')} {doc.get('Author', '')} {doc.get('Genres', '')} Rating: {doc.get('Rating', '')}\" for doc in batch]\n",
    "            description_texts = [doc.get('Description', '') if 'Description' in doc else '' for doc in batch]\n",
    "            \n",
    "            # Generate embeddings for both searchContent and Description in parallel\n",
    "            future_search = executor.submit(generate_embeddings_batch, search_texts)\n",
    "            future_description = executor.submit(generate_embeddings_batch, description_texts)\n",
    "            \n",
    "            futures[future_search] = ('Embedding', batch, search_texts, i)\n",
    "            futures[future_description] = ('DescriptionEmbedding', batch, description_texts, i)\n",
    "\n",
    "        # Process the completed futures\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                field_name, batch, texts, start_index = futures[future]\n",
    "                embeddings = future.result()\n",
    "                print(f\"Processing batch starting at index {start_index} for {field_name}\")\n",
    "\n",
    "                for j, embedding in enumerate(embeddings):\n",
    "                    # Store the embeddings in the respective fields\n",
    "                    documents[start_index + j][field_name] = embedding\n",
    "                    if field_name == 'Embedding':\n",
    "                        documents[start_index + j]['searchContent'] = texts[j]\n",
    "                    elif field_name == 'DescriptionEmbedding':\n",
    "                        documents[start_index + j]['Description'] = texts[j]  # Optional: store the text used for embedding\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch: {e}\")    \n",
    "  \n",
    "\n",
    "# Process documents to generate embeddings\n",
    "process_documents(documents)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"/lakehouse/default/Files/embeddings\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the documents with embeddings to a JSON file in the lakehouse\n",
    "output_file = os.path.join(output_dir, \"bookVectors.json\")\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(documents, file, indent=2)\n",
    "\n",
    "print(f\"Documents with embeddings saved to {output_file}\")\n",
    "\n",
    "# Upload the documents with embeddings to the index\n",
    "# search_client = SearchClient(endpoint=endpoint, index_name=search_index_name, credential=AzureKeyCredential(admin_key))\n",
    "\n",
    "# Using SearchIndexingBufferedSender to upload the documents in batches optimized for indexing\n",
    "with SearchIndexingBufferedSender(\n",
    "    endpoint=endpoint,\n",
    "    index_name=search_index_name,\n",
    "    credential=AzureKeyCredential(admin_key),\n",
    ") as batch_client:\n",
    "    # Add upload actions for all documents\n",
    "    with open(\"/lakehouse/default/Files/embeddings/bookVectors.json\", 'r') as file:\n",
    "        documents = json.load(file)\n",
    "        batch_client.upload_documents(documents=documents)\n",
    "\n",
    "print(f\"Uploaded {len(documents)} documents in total\")\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "d5a85687-f4c8-4dae-86ec-ba90dc32a717",
    "workspaceId": "9750728a-936e-41b9-a6cd-1247d645f4c5"
   },
   "lakehouse": {
    "default_lakehouse": "83b65b13-7f82-4177-838c-f19a8134860b",
    "default_lakehouse_name": "Datasets",
    "default_lakehouse_workspace_id": "9750728a-936e-41b9-a6cd-1247d645f4c5",
    "known_lakehouses": [
     {
      "id": "83b65b13-7f82-4177-838c-f19a8134860b"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
