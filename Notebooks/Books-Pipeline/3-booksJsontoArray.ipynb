{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4404b1a6-09e0-487e-a05b-23adbce1cc7f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "-**Transform Books Json to Array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086f6a2-3d8e-45d7-86cf-cc148fc5d9f4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read the newline-delimited JSON file\n",
    "input_file = '/lakehouse/default/Files/bookerset.json'\n",
    "output_file = '/lakehouse/default/Files/bookersdata.json'\n",
    "\n",
    "with open(input_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse each line as a JSON object and add to a list\n",
    "books = [json.loads(line) for line in lines]\n",
    "\n",
    "# Write the list of books to a single JSON array file\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(books, f, indent=2)\n",
    "\n",
    "print(f\"Converted {len(books)} records to a single JSON array in {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46008dd-f103-4834-88b9-3a236b43ab33",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "# Load data into pandas DataFrame from \"/lakehouse/default/\" + \"Files/bookdata.json\"\n",
    "df = pd.read_json(\"/lakehouse/default/\" + \"Files/bookersdata.json\",typ=\"series\")\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "d5a85687-f4c8-4dae-86ec-ba90dc32a717",
    "workspaceId": "9750728a-936e-41b9-a6cd-1247d645f4c5"
   },
   "lakehouse": {
    "default_lakehouse": "83b65b13-7f82-4177-838c-f19a8134860b",
    "default_lakehouse_name": "Datasets",
    "default_lakehouse_workspace_id": "9750728a-936e-41b9-a6cd-1247d645f4c5"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
