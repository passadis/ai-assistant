{"cells":[{"cell_type":"code","execution_count":null,"id":"97120cf4-2833-4d39-97ec-52838138dd2d","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"outputs":[],"source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n"]},{"cell_type":"code","execution_count":null,"id":"0739d962-5e6d-498a-8914-3166ae0dc4cc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import json\n","\n","# Define input and output file paths\n","input_file = '/lakehouse/default/Files/userdata.json'\n","output_file = '/lakehouse/default/Files/udata.json'\n","\n","# Read the JSON file\n","with open(input_file, 'r') as file:\n","    user_data = json.load(file)\n","\n","# Transform the Genres field from a JSON string to an actual list\n","for user in user_data:\n","    user['Genres'] = json.loads(user['Genres'])\n","\n","# Write the transformed data to a new JSON file\n","with open(output_file, 'w') as file:\n","    json.dump(user_data, file, indent=4)\n","\n","print(f\"Transformed data saved to {output_file}\")\n"]},{"cell_type":"code","execution_count":null,"id":"8715375c-d478-4f6c-9acd-ad3a76dc70b4","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import openai\n","import os\n","import json\n","import time\n","from azure.search.documents import SearchClient\n","from azure.core.credentials import AzureKeyCredential\n","import concurrent.futures\n","\n","# Initialize Azure OpenAI and Search clients\n","openai.api_type = \"azure\"\n","openai.api_base = \"https://xxxxxxxxxxxxxxxxxx.openai.azure.com/\"\n","openai.api_version = \"2024-02-01\"\n","openai.api_key = \"xxxxxxxxxxxxxxxxxx\"\n","\n","search_service_name = \"aiskzzgq\"\n","search_index_name = \"xxx2222-index\"\n","admin_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n","endpoint = f\"https://{search_service_name}.search.windows.net\"\n","\n","# Initialize the search client\n","search_client = SearchClient(endpoint=endpoint, index_name=search_index_name, credential=AzureKeyCredential(admin_key))\n","\n","# Load user data from JSON file\n","input_file = '/lakehouse/default/Files/udata.json'\n","with open(input_file, 'r') as file:\n","    user_data = json.load(file)\n","\n","# Function to generate embeddings for a batch of texts\n","def generate_embeddings_batch(texts, max_retries=7, backoff_factor=2):\n","    embeddings = []\n","    for text in texts:\n","        for attempt in range(max_retries):\n","            try:\n","                response = openai.Embedding.create(input=text, engine=\"text-embedding-ada-002\")  # Use the correct deployment ID\n","                embeddings.append(response['data'][0]['embedding'])\n","                break\n","            except openai.error.RateLimitError as e:\n","                if attempt < max_retries - 1:\n","                    wait_time = backoff_factor * (2 ** attempt)\n","                    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n","                    time.sleep(wait_time)\n","                else:\n","                    print(\"Max retries exceeded. Please try again later.\")\n","                    raise e\n","        time.sleep(1)  # Add a delay between individual requests to reduce aggressiveness\n","    return embeddings\n","\n","# Function to process documents in parallel\n","def process_documents(documents, batch_size=5, max_workers=8):\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = {}\n","        for i in range(0, len(documents), batch_size):\n","            batch = documents[i:i + batch_size]\n","            texts = [f\"{' '.join(doc['Genres'])} {doc['Age']}\" for doc in batch]\n","            future = executor.submit(generate_embeddings_batch, texts)\n","            futures[future] = (batch, texts, i)\n","\n","        for future in concurrent.futures.as_completed(futures):\n","            embeddings = future.result()\n","            batch, texts, start_index = futures[future]\n","            for j, embedding in enumerate(embeddings):\n","                documents[start_index + j]['contentVector'] = embedding\n","                documents[start_index + j]['searchContent'] = texts[j]\n","\n","# Generate embeddings for documents\n","process_documents(user_data)\n","\n","# Ensure the output directory exists\n","output_dir = \"/lakehouse/default/Files/embeddings\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Save the documents with embeddings to a JSON file in the lakehouse\n","output_file = os.path.join(output_dir, \"userVectors.json\")\n","with open(output_file, 'w') as file:\n","    json.dump(user_data, file)\n","\n","print(f\"Documents with embeddings saved to {output_file}\")\n","\n","# Upload the documents with embeddings to the index\n","search_client = SearchClient(endpoint=endpoint, index_name=search_index_name, credential=AzureKeyCredential(admin_key))\n","\n","# Using SearchIndexingBufferedSender to upload the documents in batches optimized for indexing\n","from azure.search.documents import SearchIndexingBufferedSender\n","\n","try:\n","    with SearchIndexingBufferedSender(\n","        endpoint=endpoint,\n","        index_name=search_index_name,\n","        credential=AzureKeyCredential(admin_key),\n","    ) as batch_client:\n","        with open(output_file, 'r') as file:\n","            documents = json.load(file)\n","            batch_client.upload_documents(documents=documents)\n","\n","    print(f\"Uploaded {len(documents)} documents in total\")\n","except Exception as e:\n","    print(f\"Error uploading documents: {e}\")\n"]}],"metadata":{"dependencies":{"environment":{"environmentId":"d5a85687-f4c8-4dae-86ec-ba90dc32a717","workspaceId":"9750728a-936e-41b9-a6cd-1247d645f4c5"},"lakehouse":{"default_lakehouse":"83b65b13-7f82-4177-838c-f19a8134860b","default_lakehouse_name":"Datasets","default_lakehouse_workspace_id":"9750728a-936e-41b9-a6cd-1247d645f4c5"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"synapse_pyspark","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
