{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file = '/lakehouse/default/Files/userdata.json'\n",
    "output_file = '/lakehouse/default/Files/udata.json'\n",
    "\n",
    "# Read the JSON file\n",
    "with open(input_file, 'r') as file:\n",
    "    user_data = json.load(file)\n",
    "\n",
    "# Transform the Genres field from a JSON string to an actual list\n",
    "for user in user_data:\n",
    "    user['Genres'] = json.loads(user['Genres'])\n",
    "\n",
    "# Write the transformed data to a new JSON file\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(user_data, file, indent=4)\n",
    "\n",
    "print(f\"Transformed data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex, SimpleField, SearchFieldDataType, SearchField,\n",
    "    VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile,\n",
    "    SemanticConfiguration, SemanticPrioritizedFields, SemanticField\n",
    ")\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import concurrent.futures\n",
    "\n",
    "# Initialize Azure OpenAI and Search clients\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://xxxxxxxxxxxxxx.openai.azure.com/\"\n",
    "openai.api_version = \"2024-02-01\"\n",
    "openai.api_key = \"xxxxxxxxxxxxxxxx\"\n",
    "\n",
    "search_service_name = \"xxxxxxxxxxxx\"\n",
    "search_index_name = \"users-index\"\n",
    "admin_key = \"xxxxxxxxxxxxxxxxx\"\n",
    "endpoint = f\"https://{search_service_name}.search.windows.net\"\n",
    "\n",
    "# Initialize the search client\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=search_index_name, credential=AzureKeyCredential(admin_key))\n",
    "\n",
    "# Define the user schema with vector and semantic search configurations\n",
    "user_fields = [\n",
    "    SimpleField(name=\"UserId\", type=SearchFieldDataType.String, key=True, retrievable=True, filterable=True),\n",
    "    SimpleField(name=\"Age\", type=SearchFieldDataType.Int32, retrievable=True, filterable=True),\n",
    "    SearchField(name=\"Genres\", type=SearchFieldDataType.String, retrievable=True, filterable=True, facetable=True, searchable=True),\n",
    "    SearchField(name=\"Embedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"userHnswProfile\")\n",
    "]\n",
    "\n",
    "# Define vector search configurations\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"userHnsw\",\n",
    "            parameters={\n",
    "                \"m\": 8,\n",
    "                \"efConstruction\": 800,\n",
    "                \"efSearch\": 800,\n",
    "                \"metric\": \"cosine\"\n",
    "            }\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"userHnswProfile\",\n",
    "            algorithm_configuration_name=\"userHnsw\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define semantic configuration\n",
    "semantic_configuration = SemanticConfiguration(\n",
    "    name=\"userSemanticConfig\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"Genres\"),\n",
    "        content_fields=[SemanticField(field_name=\"Genres\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the index schema\n",
    "index = SearchIndex(\n",
    "    name=search_index_name,\n",
    "    fields=user_fields,\n",
    "    vector_search=vector_search,\n",
    "    semantic_search_configurations=[semantic_configuration]\n",
    ")\n",
    "\n",
    "# Create or update the index\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f'Index {result.name} updated successfully')\n",
    "\n",
    "\n",
    "# Load user data from JSON file\n",
    "input_file = '/lakehouse/default/Files/udata.json'\n",
    "with open(input_file, 'r') as file:\n",
    "    user_data = json.load(file)\n",
    "\n",
    "# Function to generate embeddings for a batch of texts\n",
    "def generate_embeddings_batch(texts, max_retries=7, backoff_factor=2):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = openai.Embedding.create(input=text, engine=\"text-embedding-ada-002\")  # Use the correct deployment ID\n",
    "                embeddings.append(response['data'][0]['embedding'])\n",
    "                break\n",
    "            except openai.error.RateLimitError as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = backoff_factor * (2 ** attempt)\n",
    "                    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(\"Max retries exceeded. Please try again later.\")\n",
    "                    raise e\n",
    "        time.sleep(1)  # Add a delay between individual requests to reduce aggressiveness\n",
    "    return embeddings\n",
    "\n",
    "# Function to process documents in parallel\n",
    "def process_documents(documents, batch_size=5, max_workers=8):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {}\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i + batch_size]\n",
    "            texts = [f\"{' '.join(doc['Genres'])} {doc['Age']}\" for doc in batch]\n",
    "            future = executor.submit(generate_embeddings_batch, texts)\n",
    "            futures[future] = (batch, texts, i)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            embeddings = future.result()\n",
    "            batch, texts, start_index = futures[future]\n",
    "            for j, embedding in enumerate(embeddings):\n",
    "                documents[start_index + j]['contentVector'] = embedding\n",
    "                documents[start_index + j]['searchContent'] = texts[j]\n",
    "\n",
    "# Generate embeddings for documents\n",
    "process_documents(user_data)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir = \"/lakehouse/default/Files/embeddings\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the documents with embeddings to a JSON file in the lakehouse\n",
    "output_file = os.path.join(output_dir, \"userVectors.json\")\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(user_data, file)\n",
    "\n",
    "print(f\"Documents with embeddings saved to {output_file}\")\n",
    "\n",
    "# Upload the documents with embeddings to the index\n",
    "search_client = SearchClient(endpoint=endpoint, index_name=search_index_name, credential=AzureKeyCredential(admin_key))\n",
    "\n",
    "# Using SearchIndexingBufferedSender to upload the documents in batches optimized for indexing\n",
    "from azure.search.documents import SearchIndexingBufferedSender\n",
    "\n",
    "try:\n",
    "    with SearchIndexingBufferedSender(\n",
    "        endpoint=endpoint,\n",
    "        index_name=search_index_name,\n",
    "        credential=AzureKeyCredential(admin_key),\n",
    "    ) as batch_client:\n",
    "        with open(output_file, 'r') as file:\n",
    "            documents = json.load(file)\n",
    "            batch_client.upload_documents(documents=documents)\n",
    "\n",
    "    print(f\"Uploaded {len(documents)} documents in total\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading documents: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
