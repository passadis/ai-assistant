{"cells":[{"cell_type":"markdown","source":["### Users Notebook\n","- This Notebook creates new AI Search Index\n","- Creates Vector Profile\n","- Gets the user JSON and transforms as needed\n","- Uploads the Users to the Index\n","- Generates embeddings and stores them to the Index as well\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6d29c1da-fab7-4aea-a122-628ec04f34d9"},{"cell_type":"code","source":["import json\n","\n","# Define input and output file paths\n","input_file = '/lakehouse/default/Files/userdata.json'\n","output_file = '/lakehouse/default/Files/udata.json'\n","\n","# Read the JSON file\n","with open(input_file, 'r') as file:\n","    user_data = json.load(file)\n","\n","# Transform the Genres field from a JSON string to an actual list\n","for user in user_data:\n","    try:\n","        user['Genres'] = json.loads(user['Genres'])\n","    except json.JSONDecodeError as e:\n","        print(f\"Invalid JSON format for Genres in user ID {user.get('id', 'unknown')}: {user['Genres']}. Error: {e}\")\n","        user['Genres'] = []  # Set to an empty list or handle it as per your requirement\n","\n","# Write the transformed data to a new JSON file\n","with open(output_file, 'w') as file:\n","    json.dump(user_data, file, indent=4)\n","\n","print(f\"Transformed data saved to {output_file}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0739d962-5e6d-498a-8914-3166ae0dc4cc"},{"cell_type":"code","source":["import openai\n","import json\n","import time\n","import os\n","from azure.search.documents import SearchClient\n","from azure.search.documents.indexes import SearchIndexClient\n","from azure.search.documents.indexes.models import (\n","    SimpleField, SearchFieldDataType, SearchField, SearchIndex,\n","    VectorSearch, HnswAlgorithmConfiguration, VectorSearchProfile,\n","    SemanticConfiguration, SemanticPrioritizedFields, SemanticField, SemanticSearch\n",")\n","from azure.core.credentials import AzureKeyCredential\n","import concurrent.futures\n","from azure.search.documents import SearchIndexingBufferedSender\n","\n","# Initialize Azure OpenAI and Search clients\n","openai.api_type = \"azure\"\n","openai.api_base = \"https://dev-oai-kpass.openai.azure.com/\"\n","openai.api_version = \"2024-02-01\"\n","openai.api_key = \"18a8d8f39a794c169130055ac2c2ff7d\"\n","\n","search_service_name = \"azaivztqx\"\n","search_index_name = \"users-index\"\n","admin_key = \"UvNc9RS47BkkZi0Hz7XPdSkpvi9QXDuqbg6rrejGw5AzSeBxWhxe\"\n","endpoint = f\"https://{search_service_name}.search.windows.net\"\n","\n","# Initialize the search client\n","search_client = SearchClient(endpoint=endpoint, index_name=search_index_name, credential=AzureKeyCredential(admin_key))\n","\n","# Define the initial user schema without the Embedding field\n","user_fields = [\n","    SimpleField(name=\"UserId\", type=SearchFieldDataType.String, key=True, retrievable=True, filterable=True),\n","    SimpleField(name=\"Age\", type=SearchFieldDataType.Int32, retrievable=True, filterable=True),\n","    SearchField(name=\"Genres\", type=SearchFieldDataType.Collection(SearchFieldDataType.String), retrievable=True, filterable=True, facetable=True, searchable=True)\n","]\n","\n","# Define vector search configurations\n","vector_search = VectorSearch(\n","    algorithms=[\n","        HnswAlgorithmConfiguration(\n","            name=\"userHnsw\",\n","            parameters={\n","                \"m\": 8,\n","                \"efConstruction\": 800,\n","                \"efSearch\": 800,\n","                \"metric\": \"cosine\"\n","            }\n","        )\n","    ],\n","    profiles=[\n","        VectorSearchProfile(\n","            name=\"userHnswProfile\",\n","            algorithm_configuration_name=\"userHnsw\",\n","        )\n","    ]\n",")\n","\n","# Define semantic configuration\n","semantic_configurations = SemanticConfiguration(\n","    name=\"users-semantic-config\",\n","    prioritized_fields=SemanticPrioritizedFields(\n","        title_field=SemanticField(field_name=\"\"),\n","        keywords_fields=[SemanticField(field_name=\"Genres\")],\n","        content_fields=[SemanticField(field_name=\"searchContent\")]\n","    )\n",")\n","\n","# Create the initial index schema\n","index = SearchIndex(\n","    name=search_index_name,\n","    fields=user_fields,\n","    vector_search=vector_search,\n","    semantic_configurations=[semantic_configurations]\n",")\n","\n","# Create a search index client\n","index_client = SearchIndexClient(endpoint=endpoint, credential=AzureKeyCredential(admin_key))\n","\n","# Create or update the index\n","result = index_client.create_or_update_index(index)\n","print(f'Index {result.name} created or updated successfully')\n","\n","# Add the Embedding and searchContent fields to the existing index\n","additional_fields = [\n","    SearchField(name=\"Embedding\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), retrievable=True, searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"userHnswProfile\"),\n","    SearchField(name=\"searchContent\", type=SearchFieldDataType.String, searchable=True)\n","]\n","\n","# Retrieve the existing index\n","existing_index = index_client.get_index(search_index_name)\n","existing_index.fields.extend(additional_fields)\n","\n","# Update the index with the new fields\n","result = index_client.create_or_update_index(existing_index)\n","print(f'Index {result.name} updated with additional fields')\n","\n","# Load user data from JSON file\n","input_file = '/lakehouse/default/Files/udata.json'\n","with open(input_file, 'r') as file:\n","    user_data = json.load(file)\n","\n","# Lists to hold valid and invalid documents\n","valid_documents = []\n","invalid_documents = []\n","\n","# Validate each document\n","for doc in user_data:\n","    valid = True\n","\n","    if 'UserId' in doc:\n","        try:\n","            doc['UserId'] = str(doc['UserId'])\n","        except ValueError:\n","            print(f\"Invalid value for UserId in document ID {doc['UserId']}: {doc['UserId']}\")\n","            invalid_documents.append(doc)\n","            valid = False\n","    \n","    # Validate Genres is a list of strings\n","    if 'Genres' in doc:\n","        if isinstance(doc['Genres'], str):\n","            try:\n","                doc['Genres'] = json.loads(doc['Genres'])\n","            except json.JSONDecodeError:\n","                print(f\"Invalid JSON format for Genres in document ID {doc['UserId']}: {doc['Genres']}\")\n","                invalid_documents.append(doc)\n","                valid = False\n","        elif isinstance(doc['Genres'], list):\n","            if not all(isinstance(genre, str) for genre in doc['Genres']):\n","                print(f\"Unexpected format for Genres in document ID {doc['UserId']}: {doc['Genres']}\")\n","                invalid_documents.append(doc)\n","                valid = False\n","        else:\n","            print(f\"Unexpected format for Genres in document ID {doc['UserId']}: {doc['Genres']}\")\n","            invalid_documents.append(doc)\n","            valid = False\n","    \n","    if valid:\n","        valid_documents.append(doc)\n","\n","# Log the number of valid and invalid documents\n","print(f\"Valid documents: {len(valid_documents)}\")\n","print(f\"Invalid documents: {len(invalid_documents)}\")\n","\n","# Upload valid documents to the Azure Search index\n","if valid_documents:\n","    result = search_client.upload_documents(documents=valid_documents)\n","    print(f\"Uploaded {len(valid_documents)} documents to the Azure Search index. Results: {result}\")\n","else:\n","    print(\"No valid documents to upload.\")\n","\n","# Function to generate embeddings for a batch of texts\n","def generate_embeddings_batch(texts, max_retries=7, backoff_factor=2):\n","    embeddings = []\n","    for text in texts:\n","        for attempt in range(max_retries):\n","            try:\n","                response = openai.Embedding.create(input=text, engine=\"text-embedding-ada-002\")  # Use the correct deployment ID\n","                embeddings.append(response['data'][0]['embedding'])\n","                break\n","            except openai.error.RateLimitError as e:\n","                if attempt < max_retries - 1:\n","                    wait_time = backoff_factor * (2 ** attempt)\n","                    print(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n","                    time.sleep(wait_time)\n","                else:\n","                    print(\"Max retries exceeded. Please try again later.\")\n","                    raise e\n","        time.sleep(1)  # Add a delay between individual requests to reduce aggressiveness\n","    return embeddings\n","\n","# Function to process documents in parallel\n","def process_documents(documents, batch_size=5, max_workers=8):\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = {}\n","        for i in range(0, len(documents), batch_size):\n","            batch = documents[i:i + batch_size]\n","            texts = [f\"{' '.join(doc['Genres'])} {doc['Age']}\" for doc in batch]\n","            future = executor.submit(generate_embeddings_batch, texts)\n","            futures[future] = (batch, texts, i)\n","\n","        for future in concurrent.futures.as_completed(futures):\n","            embeddings = future.result()\n","            batch, texts, start_index = futures[future]\n","            for j, embedding in enumerate(embeddings):\n","                documents[start_index + j]['Embedding'] = embedding\n","                documents[start_index + j]['searchContent'] = texts[j]\n","\n","# Generate embeddings for documents\n","process_documents(valid_documents)\n","\n","# Ensure the output directory exists\n","output_dir = \"/lakehouse/default/Files/embeddings\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Save the documents with embeddings to a JSON file in the lakehouse\n","output_file = os.path.join(output_dir, \"userVectors.json\")\n","with open(output_file, 'w') as file:\n","    json.dump(valid_documents, file)\n","\n","print(f\"Documents with embeddings saved to {output_file}\")\n","\n","# Upload the documents with embeddings to the index\n","try:\n","    with SearchIndexingBufferedSender(\n","        endpoint=endpoint,\n","        index_name=search_index_name,\n","        credential=AzureKeyCredential(admin_key),\n","    ) as batch_client:\n","        with open(output_file, 'r') as file:\n","            documents = json.load(file)\n","            batch_client.upload_documents(documents=documents)\n","\n","    print(f\"Uploaded {len(documents)} documents in total\")\n","except Exception as e:\n","    print(f\"Error uploading documents: {e}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"livy_statement_state":"available","session_id":"f33da179-175f-4cec-b466-d235b2f2978e","state":"finished","normalized_state":"finished","queued_time":"2024-07-25T01:44:00.5018401Z","session_start_time":"2024-07-25T01:44:00.982386Z","execution_start_time":"2024-07-25T01:45:12.3989769Z","execution_finish_time":"2024-07-25T01:45:21.3855698Z","parent_msg_id":"9fb1034a-7128-4494-8dac-9676a3b3dd7d"},"text/plain":"StatementMeta(, f33da179-175f-4cec-b466-d235b2f2978e, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Index users-index created or updated successfully\nIndex users-index updated with additional fields\nValid documents: 5\nInvalid documents: 0\nUploaded 5 documents to the Azure Search index. Results: [<azure.search.documents._generated.models._models_py3.IndexingResult object at 0x7bdc74dc3670>, <azure.search.documents._generated.models._models_py3.IndexingResult object at 0x7bdc74dc38b0>, <azure.search.documents._generated.models._models_py3.IndexingResult object at 0x7bdc74dc3910>, <azure.search.documents._generated.models._models_py3.IndexingResult object at 0x7bdc74dc3970>, <azure.search.documents._generated.models._models_py3.IndexingResult object at 0x7bdc74dc39d0>]\nDocuments with embeddings saved to /lakehouse/default/Files/embeddings/userVectors.json\nUploaded 5 documents in total\n"]}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8715375c-d478-4f6c-9acd-ad3a76dc70b4"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"83b65b13-7f82-4177-838c-f19a8134860b","default_lakehouse_name":"Datasets","default_lakehouse_workspace_id":"9750728a-936e-41b9-a6cd-1247d645f4c5"},"environment":{"environmentId":"d5a85687-f4c8-4dae-86ec-ba90dc32a717","workspaceId":"9750728a-936e-41b9-a6cd-1247d645f4c5"}}},"nbformat":4,"nbformat_minor":5}